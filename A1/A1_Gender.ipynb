{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script was run from Google colab, so it was necessary to mount my Google drive to access the image files \n",
    "#and save the resulting path\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install package to unzip .rar file\n",
    "!pip install unrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip the files\n",
    "!unrar x '/content/drive/My Drive/celeba.rar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path\n",
    "PATH = '/content/celeba/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "#Read in the csv data\n",
    "data = pd.read_csv(PATH + 'labels.csv', delim_whitespace=True)\n",
    "#Create an index\n",
    "data.set_index('img_name', inplace=True)\n",
    "#Instead of values 1,-1 I set 0,1 (replace all -1 with 0)\n",
    "data.replace(-1,0, inplace= True)\n",
    "#Read in to pickle file\n",
    "data.to_pickle(PATH + 'data_pkl.pkl')\n",
    "\n",
    "# for i in ['train', 'valid']:\n",
    "#     os.mkdir(os.path.join(PATH , i))\n",
    "\n",
    "#Find the image files/names\n",
    "filenames = glob.glob(PATH + 'img/*jpg')\n",
    "#Random shuffle so avoid biased training\n",
    "shuffle = np.random.permutation(len(filenames))\n",
    "\n",
    "#Create dataframes\n",
    "training_df = pd.DataFrame()\n",
    "valid_df = pd.DataFrame()\n",
    "\n",
    "#Create seperate folders for training and validation & show progress bar\n",
    "\n",
    "#Split 4,500 image files into training folder\n",
    "for j in tqdm(shuffle[:4500]):\n",
    "    file = filenames[j].split('/')[-1]\n",
    "    training_df = training_df.append( data[data.index == file])\n",
    "    shutil.copy(PATH + 'img/' + file, PATH + 'train/' + file)\n",
    "\n",
    "#Split the remaining 500 image files into validation folder\n",
    "for j in tqdm(shuffle[4500:]):\n",
    "    file = filenames[j].split('/')[-1]\n",
    "    valid_df = valid_df.append(data[data.index == file])\n",
    "    shutil.copy(PATH +'img/'+ file, PATH + 'valid/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create csv and pickle files \n",
    "training_df.to_csv(PATH + 'train.csv')\n",
    "training_df.to_pickle((PATH + 'train.pkl'))\n",
    "\n",
    "valid_df.to_csv(PATH + 'valid.csv')\n",
    "valid_df.to_pickle(PATH + 'valid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import os\n",
    "\n",
    "#Outsource computing power to Google servers to avoid computer damage\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define a class that will load the data when called\n",
    "class Gender_loader(Dataset):\n",
    "    def __init__(self, df, img_dir, transform = None):\n",
    "        self.dataframe = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.filename = df.index\n",
    "        self.label = df.gender.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(os.path.join(self.img_dir, self.filename[idx]))\n",
    "        label = self.label[idx]\n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            image = self.transform(sample['image'])\n",
    "            sample = {'image': image, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pre-constructed model architectures accessed via urls\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19',\n",
    "]\n",
    "\n",
    "model_urls = {\n",
    "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
    "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
    "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
    "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
    "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
    "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
    "}\n",
    "\n",
    "#Create VGG model class which specifies parameters, and a classifier\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, num_classes=2, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(4096, 2),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#Define CNN layers\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "#Define batch sizes\n",
    "cfgs = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "#Define models\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 11-layer model (configuration \"A\") from\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 13-layer model (configuration \"B\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg13', 'B', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg13_bn', 'B', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg19', 'E', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg19_bn', 'E', True, pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision.utils import make_grid\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "#from Models import vgg13_bn, vgg11_bn\n",
    "from imgaug import augmenters as iaa\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "#Class specifying image transforms to improve performancce of model\n",
    "class ImAugtransforms:\n",
    "    def __init__(self):\n",
    "        self.aug = iaa.Sequential([\n",
    "            iaa.Sometimes(0.3, iaa.GaussianBlur(sigma=(0, 2.0))),\n",
    "            iaa.Affine(rotate=(-30, 30), mode='symmetric'),\n",
    "            iaa.Sometimes(0.25,\n",
    "                          iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n",
    "                                     iaa.CoarseDropout(0.1, size_percent=0.5)])),\n",
    "            iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        return self.aug.augment_image(img)\n",
    "\n",
    "#More transformations\n",
    "train_trns = transforms.Compose([\n",
    "    ImAugtransforms(),\n",
    "    lambda x: PIL.Image.fromarray(x),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.7),\n",
    "    transforms.RandomGrayscale(p=0.35),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "valid_trns = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the training dataframe\n",
    "training_df = pd.read_pickle(PATH +'train.pkl')\n",
    "training_directory = PATH + 'train/'\n",
    "\n",
    "#Read the validation dataframe\n",
    "valid_df = pd.read_pickle(PATH + 'valid.pkl')\n",
    "validation_directory = PATH + 'valid/'\n",
    "\n",
    "#Use gender_loader class to call images\n",
    "training_dataloader = Gender_loader(training_df, training_directory, transform=train_trns)\n",
    "validation_dataloader = Gender_loader(valid_df, validation_directory, transform=valid_trns)\n",
    "\n",
    "#Plot three images to check transformations\n",
    "plt.imshow(make_grid(training_dataloader[1]['image'].permute(1, 2, 0)))\n",
    "plt.show()\n",
    "plt.imshow(make_grid(training_dataloader[7]['image'].permute(1, 2, 0)))\n",
    "plt.show()\n",
    "plt.imshow(make_grid(training_dataloader[9]['image'].permute(1, 2, 0)))\n",
    "plt.show()\n",
    "print(training_dataloader[1]['label'])\n",
    "print(training_dataloader[7]['label'])\n",
    "print(training_dataloader[9]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify model - use vgg19 as it is most accurate\n",
    "model = vgg19_bn()\n",
    "model.to(device)\n",
    "#Specify batch size, epochs\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "\n",
    "#Set-up training and validation dataloaders\n",
    "train_dl = DataLoader(training_dataloader, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(validation_dataloader, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "#Specify optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1,\n",
    "                       amsgrad=False)\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model function\n",
    "def fit_model(epochs, model, dataloader, phase='training', volatile=False):\n",
    "    pprint(\"Epoch: {}\".format(epochs))\n",
    "\n",
    "    #If training phase, train model\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    #If validation phase, evaluate accuracy and loss of validation image set with trained model\n",
    "    if phase == 'validataion':\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "\n",
    "    running_loss = []\n",
    "    running_acc = []\n",
    "    b = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "      \n",
    "        inputs, target = data['image'].cuda(), data['label'].cuda()\n",
    "\n",
    "        inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "        #If training phase, train the model, evaluate and print the loss and accuracy\n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "\n",
    "        outputs = outputs.to(device=device, dtype=torch.float32)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        acc_ = []\n",
    "\n",
    "        accuracy = (get_num_correct(outputs, target)/BATCH_SIZE)\n",
    "        acc_.append(accuracy)\n",
    "\n",
    "        print('In Epoch', epochs)\n",
    "        print('')\n",
    "        print('predictions', preds)\n",
    "        print('targets    ', target)\n",
    "        print('Batch Accuracy is ' + \"{:.2%}\".format(accuracy))\n",
    "        print('')\n",
    "        print('Batch Loss is     ', loss)\n",
    "        print('')        \n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        running_acc.append(np.asarray(acc_).mean())\n",
    "\n",
    "        b += 1\n",
    "\n",
    "        #If training phase, send loss backwards so model can 'learn'\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    total_batch_loss = np.asarray(running_loss).mean()\n",
    "    total_batch_acc = np.asarray(running_acc).mean()\n",
    "\n",
    "    pprint(\"{} loss is {} \".format(phase, total_batch_loss))\n",
    "    pprint(\"{} accuracy is {} \".format(phase, total_batch_acc))\n",
    "\n",
    "\n",
    "    return total_batch_loss, total_batch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set-up empty lists to improve performance\n",
    "trn_losses = [];\n",
    "trn_acc = []\n",
    "val_losses = [];\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model and display progress\n",
    "for i in tqdm(range(1, EPOCHS +1)):\n",
    "    trn_l, trn_a = fit_model(i, model, train_dl)\n",
    "    val_l, val_a = fit_model(i, model, valid_dl, phase='validation')\n",
    "    trn_losses.append(trn_l);\n",
    "    trn_acc.append(trn_a)\n",
    "    val_losses.append(val_l);\n",
    "    val_acc.append(val_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot and save training and validation losses\n",
    "%matplotlib inline\n",
    "plt.plot(trn_losses, label = 'training')\n",
    "plt.plot(val_losses, label = 'validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('/content/drive/My Drive/A1_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot and save training and validation accuracies\n",
    "plt.plot(trn_acc, label = 'training')\n",
    "plt.plot(val_acc, label = 'validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('/content/drive/My Drive/A1_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model path so it can be called to evaluate new images\n",
    "save_path = os.path.join(PATH + 'A1.pth')\n",
    "torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify transforms for image predict function\n",
    "model.eval()\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                 ])\n",
    "\n",
    "#Create predict image class that can be called from main.py\n",
    "def predict(image):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    image_tensor = image_tensor.to(device=device)\n",
    "    outputs = model(image_tensor)\n",
    "    outputs = outputs.to(device=device, dtype=torch.float32)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    # index = output.data.cpu().numpy().argmax()\n",
    "    return preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
