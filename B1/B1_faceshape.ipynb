{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import PIL \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Data = pd.read_csv('/content/cartoon_set/labels.csv', delim_whitespace= True)\n",
    "Data.set_index('file_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "PATH = '/content/cartoon_set/img/'\n",
    "filenames = glob.glob(PATH + '*png')\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['train', 'valid']:\n",
    "  for j in ['0','1','2','3','4']:\n",
    "    if not os.path.exists(os.path.join(PATH, i, j)):\n",
    "      os.makedirs(os.path.join(PATH, i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "shuffle = np.random.permutation(len(filenames))\n",
    "for j in tqdm(shuffle[:9000]):\n",
    "  file = filenames[j].split('/')[-1]\n",
    "  #print(file)\n",
    "  if Data[Data.index == file]['face_shape'].item() == 0:\n",
    "    shutil.copy(PATH + file, PATH + 'train/' +'0/' + file)\n",
    "  \n",
    "  elif Data[Data.index == file]['face_shape'].item() == 1:\n",
    "    shutil.copy(PATH + file, PATH + 'train/' +'1/' + file)\n",
    "\n",
    "  elif Data[Data.index == file]['face_shape'].item() == 2:\n",
    "    shutil.copy(PATH + file, PATH + 'train/' +'2/' + file)\n",
    "\n",
    "  elif Data[Data.index == file]['face_shape'].item() == 3:\n",
    "    shutil.copy(PATH + file, PATH + 'train/' +'3/' + file)\n",
    "\n",
    "  elif Data[Data.index == file]['face_shape'].item() == 4:\n",
    "    shutil.copy(PATH + file, PATH + 'train/' +'4/' + file)\n",
    "\n",
    "for i in tqdm(shuffle[9000:]):\n",
    "  file = filenames[i].split('/')[-1]\n",
    "  #print(file)\n",
    "  if Data[Data.index == file]['face_shape'].item() == 0:\n",
    "    shutil.copy(PATH + file, PATH + 'valid/' +'0/' + file)\n",
    "  \n",
    "  elif Data[Data.index == file]['face_shape'].item() == 1:\n",
    "    shutil.copy(PATH + file, PATH + 'valid/' +'1/' + file)\n",
    "\n",
    "  elif Data[Data.index == file]['face_shape'].item() == 2:\n",
    "    shutil.copy(PATH + file, PATH + 'valid/' +'2/' + file)\n",
    "\n",
    "  elif Data[Data.index == file]['face_shape'].item() == 3:\n",
    "    shutil.copy(PATH + file, PATH + 'valid/' +'3/' + file)\n",
    "\n",
    "  elif Data[Data.index == file]['face_shape'].item() == 4:\n",
    "    shutil.copy(PATH + file, PATH + 'valid/' +'4/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/content/cartoon_set/img/'\n",
    "input_size = 400\n",
    "batch_size = 10\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ColorJitter(brightness=0, contrast=0.7, saturation=0.7, hue=0),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomRotation(degrees=15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=7) for x in ['train', 'valid']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainiter = iter(dataloaders_dict['train'])\n",
    "features, labels = next(trainiter)\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for d in os.listdir('/content/cartoon_set/img/train/'):\n",
    "    categories.append(d)\n",
    "    \n",
    "n_classes = len(categories)\n",
    "print(f'There are {n_classes} different classes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = image_datasets['train'].class_to_idx\n",
    "idx_to_class = {\n",
    "    idx: class_\n",
    "    for class_, idx in image_datasets['train'].class_to_idx.items()\n",
    "}\n",
    "\n",
    "\n",
    "train_cnts = Counter([idx_to_class[x] for x in image_datasets['train'].targets])\n",
    "val_cnts = Counter([idx_to_class[x] for x in  image_datasets['valid'].targets])\n",
    "\n",
    "train_cnts = pd.DataFrame({'cat' :list(train_cnts.keys()), 'train_cnt': list(train_cnts.values())})\n",
    "val_cnts = pd.DataFrame({'cat' :list(val_cnts.keys()), 'val_cnt': list(val_cnts.values())})\n",
    "\n",
    "\n",
    "cnt_df = pd.merge(train_cnts,val_cnts,on='cat',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "plt.imshow(make_grid(features).permute(1, 2, 0))\n",
    "plt.show()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model = models.vgg16(pretrained=True)\n",
    "model2 = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "\n",
    "  param.requires_grad = False\n",
    "\n",
    "# for param in model.classifier[6].parameters():\n",
    "#   param.requires_grad = False\n",
    "\n",
    "\n",
    "for param in model2.parameters():\n",
    "  param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = model.classifier[6].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = model2.fc.in_features\n",
    "\n",
    "\n",
    "model2.fc = nn.Sequential(nn.Linear(num_inputs, n_classes))\n",
    "\n",
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 2048), nn.ReLU(), nn.Dropout(0.65),\n",
    "    nn.Linear(2048, 1024),nn.ReLU(),\n",
    "    nn.Dropout(0.6),\n",
    "    nn.Linear(1024, 512),nn.ReLU(),\n",
    "    nn.Linear(512, n_classes))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model= model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.01,momentum = 0.9)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer,3,verbose=True, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def train(model,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          save_file_name,\n",
    "          scheduler,\n",
    "          max_epochs_stop=3,\n",
    "          n_epochs=30,\n",
    "          print_every=1,\n",
    "          ):\n",
    "    \"\"\"Train a PyTorch Model\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        model (PyTorch model): cnn to train\n",
    "        criterion (PyTorch loss): objective to minimize\n",
    "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
    "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
    "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
    "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
    "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
    "        n_epochs (int): maximum number of training epochs\n",
    "        print_every (int): frequency of epochs to print training stats\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        model (PyTorch model): trained cnn with best weights\n",
    "        history (DataFrame): history of train and validation loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Early stopping intialization\n",
    "    epochs_no_improve = 0\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    valid_max_acc = 0\n",
    "    history = []\n",
    "\n",
    "    # Number of epochs already trained (if using loaded in model weights)\n",
    "    try:\n",
    "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
    "    except:\n",
    "        model.epochs = 0\n",
    "        print(f'Starting Training from Scratch.\\n')\n",
    "\n",
    "    overall_start = timer()\n",
    "\n",
    "    # Main loop\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "        # keep track of training and validation loss each epoch\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_acc = 0\n",
    "        valid_acc = 0\n",
    "\n",
    "        # Set to training\n",
    "        model.train()\n",
    "        start = timer()\n",
    "\n",
    "       \n",
    "\n",
    "        # Training loop\n",
    "        for ii, (data, target) in enumerate(train_loader):\n",
    "            # Tensors to gpu\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Predicted outputs are log probabilities\n",
    "            output = model(data)\n",
    "\n",
    "            # Loss and backpropagation of gradients\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            # Track train loss by multiplying average loss by number of examples in batch\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Calculate accuracy by finding max log probability\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            # Need to convert correct tensor from int to float to average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "            # Multiply average accuracy times the number of examples in batch\n",
    "            train_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "\n",
    "\n",
    "            # Track training progress\n",
    "            print(\n",
    "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
    "                end='\\r')\n",
    "\n",
    "        # After training loops ends, start validation\n",
    "        else:\n",
    "            \n",
    "            model.epochs += 1\n",
    "\n",
    "            # Don't need to keep track of gradients\n",
    "            with torch.no_grad():\n",
    "                # Set to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                # Validation loop\n",
    "                for data, target in valid_loader:\n",
    "                    # Tensors to gpu\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                    # Forward pass\n",
    "                    output = model(data)\n",
    "\n",
    "                    # Validation loss\n",
    "                    loss = criterion(output, target)\n",
    "                    # Multiply average loss times the number of examples in batch\n",
    "                    valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                    # Calculate validation accuracy\n",
    "                    _, pred = torch.max(output, dim=1)\n",
    "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                    accuracy = torch.mean(\n",
    "                        correct_tensor.type(torch.FloatTensor))\n",
    "                    # Multiply average accuracy times the number of examples\n",
    "                    valid_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "                # Calculate average losses\n",
    "                train_loss = train_loss / len(train_loader.dataset)\n",
    "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "                # Calculate average accuracy\n",
    "                train_acc = train_acc / len(train_loader.dataset)\n",
    "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
    "                scheduler.step()\n",
    "\n",
    "                \n",
    "\n",
    "                  \n",
    "\n",
    "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
    "\n",
    "                # Print training and validation results\n",
    "                if (epoch + 1) % print_every == 0:\n",
    "                    print(\n",
    "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
    "                    )\n",
    "                    print(\n",
    "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
    "                    )\n",
    "\n",
    "\n",
    "                # Save the model if validation loss decreases\n",
    "                if valid_loss < valid_loss_min:\n",
    "                    # Save model\n",
    "                    torch.save(model.state_dict(), save_file_name)\n",
    "                    # Track improvement\n",
    "                    epochs_no_improve = 0\n",
    "                    valid_loss_min = valid_loss\n",
    "                    valid_best_acc = valid_acc\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                # Otherwise increment count of epochs with no improvement\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    # Trigger early stopping\n",
    "                    if epochs_no_improve >= max_epochs_stop:\n",
    "                        print(\n",
    "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "                        )\n",
    "                        total_time = timer() - overall_start\n",
    "                        print(\n",
    "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
    "                        )\n",
    "\n",
    "                        # Load the best state dict\n",
    "                        model.load_state_dict(torch.load(save_file_name))\n",
    "                        # Attach the optimizer\n",
    "                        model.optimizer = optimizer\n",
    "\n",
    "                        # Format history\n",
    "                        history = pd.DataFrame(\n",
    "                            history,\n",
    "                            columns=[\n",
    "                                'train_loss', 'valid_loss', 'train_acc',\n",
    "                                'valid_acc'\n",
    "                            ])\n",
    "                        return model, history\n",
    "\n",
    "    # Attach the optimizer\n",
    "    model.optimizer = optimizer\n",
    "    # Record overall time and print out stats\n",
    "    total_time = timer() - overall_start\n",
    "    print(\n",
    "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "    )\n",
    "    print(\n",
    "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
    "    )\n",
    "    # Format history\n",
    "    history = pd.DataFrame(\n",
    "        history,\n",
    "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalid(model=None, criterion=None, optimizer=None, dataset=None, k_fold=5):\n",
    "    train_score = pd.Series()\n",
    "    val_score = pd.Series()\n",
    "\n",
    "    total_size = len(dataset)\n",
    "    fraction = 1 / k_fold\n",
    "    seg = int(total_size * fraction)\n",
    "    # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset\n",
    "    # index: [trll,trlr],[vall,valr],[trrl,trrr]\n",
    "    for i in range(k_fold):\n",
    "        trll = 0\n",
    "        trlr = i * seg\n",
    "        vall = trlr\n",
    "        valr = i * seg + seg\n",
    "        trrl = valr\n",
    "        trrr = total_size\n",
    "        # msg\n",
    "        #         print(\"train indices: [%d,%d),[%d,%d), test indices: [%d,%d)\"\n",
    "        #               % (trll,trlr,trrl,trrr,vall,valr))\n",
    "\n",
    "        train_left_indices = list(range(trll, trlr))\n",
    "        train_right_indices = list(range(trrl, trrr))\n",
    "\n",
    "        train_indices = train_left_indices + train_right_indices\n",
    "        val_indices = list(range(vall, valr))\n",
    "\n",
    "        train_set = torch.utils.data.dataset.Subset(dataset, train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(dataset, val_indices)\n",
    "\n",
    "        #         print(len(train_set),len(val_set))\n",
    "        #         print()\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=50,\n",
    "                                                   shuffle=True, num_workers=4)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=50,\n",
    "                                                 shuffle=True, num_workers=4)\n",
    "        train_acc = train(res_model, criterion, optimizer, train_loader, epoch=1)\n",
    "        train_score.at[i] = train_acc\n",
    "        val_acc = valid(res_model, criterion, optimizer, val_loader)\n",
    "        val_score.at[i] = val_acc\n",
    "\n",
    "    return train_score, val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [8]\n",
    "learning_rates = [0.001]\n",
    "momentums = [0.9]\n",
    "model_list =['resnet']\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "output_dict={}\n",
    "num_epochs = 10\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "  for learning_rate in learning_rates:\n",
    "    for momentum in momentums:\n",
    "      for model_choice in model_list:\n",
    "      \n",
    "        dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=5) for x in ['train', 'valid']}\n",
    "\n",
    "        if model_choice == 'vgg':\n",
    "          model = models.vgg16(pretrained=True)\n",
    "\n",
    "          for param in model.parameters():\n",
    "\n",
    "            param.requires_grad = False\n",
    "\n",
    "          n_inputs = model.classifier[6].in_features\n",
    "          model.classifier[6] = nn.Sequential(\n",
    "          nn.Linear(n_inputs, 2048), nn.ReLU(), nn.Dropout(0.65),\n",
    "          nn.Linear(2048, 1024),nn.ReLU(),\n",
    "          nn.Dropout(0.6),\n",
    "          nn.Linear(1024, 512),nn.ReLU(),\n",
    "          nn.Linear(512, n_classes))\n",
    "\n",
    "          model = model.to(device)\n",
    "\n",
    "\n",
    "        if model_choice == 'resnet':\n",
    "\n",
    "          model = models.resnet50(pretrained=True)\n",
    "\n",
    "          for param in model.parameters():\n",
    "\n",
    "            param.requires_grad = True\n",
    "\n",
    "          num_inputs = model.fc.in_features\n",
    "          model.fc = nn.Sequential(nn.Linear(num_inputs, n_classes))\n",
    "          model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.01,momentum = momentum)\n",
    "        exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer,3,gamma=1)\n",
    "\n",
    "\n",
    "        NEWPATH = '/content/drive/My Drive/'\n",
    "        save_path = os.path.join(NEWPATH + 'ResnetB1.pth')\n",
    "        checkpoint_path = NEWPATH\n",
    "\n",
    "        model,history = train(model,criterion,optimizer,dataloaders_dict['train'],\n",
    "              dataloaders_dict['valid'],\n",
    "              save_path,\n",
    "              exp_lr_scheduler,\n",
    "              max_epochs_stop = 10, n_epochs = num_epochs,\n",
    "              )\n",
    "        \n",
    "        print('')\n",
    "        print('Batch Size', batch_size, 'Learning_rate', learning_rate)\n",
    "        print('')\n",
    "        print('Momentum', momentum, 'model', model_choice)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for c in ['train_loss', 'valid_loss']:\n",
    "            plt.plot(\n",
    "                history[c], label=c)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Loss')\n",
    "        plt.title('Training and Validation Losses')\n",
    "        plt.savefig('/content/drive/My Drive/B1_loss' + str(batch_size) + str(learning_rate)+ str(momentum)+ model_choice + '.png')\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for c in ['train_acc', 'valid_acc']:\n",
    "            plt.plot(\n",
    "                100 * history[c], label=c)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.savefig('/content/drive/My Drive/B1_acc' + str(batch_size) + str(learning_rate)+ str(momentum)+ model_choice + '.png')\n",
    "\n",
    "\n",
    "        output_dict[str(batch_size) + str(learning_rate) + str(momentum) + model_choice] = history\n",
    "\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
